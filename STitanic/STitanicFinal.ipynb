{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "test_data = pd. read_csv(\"./test.csv\")\n",
    "\n",
    "sub_ids = test_data[\"PassengerId\"].to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_in_train = sum([True for idx,row in train_data.iterrows() if any(row.isnull())])\n",
    "missing_in_test = sum([True for idx,row in test_data.iterrows() if any(row.isnull())])\n",
    "\n",
    "f\"There are {missing_in_train} missing rows in train and {missing_in_test} missing rows in test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frame_ready(source_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "\n",
    "    #make new column group from the passengerId\n",
    "    source_frame[\"Group\"] = source_frame[\"PassengerId\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    #make new column family from the name of the passenger\n",
    "    source_frame[\"Family\"] = source_frame[\"Name\"].apply(lambda x: str(x).split(\" \")[-1])\n",
    "\n",
    "\n",
    "    #impute missing family from group\n",
    "    #source_frame[\"Family\"] = source_frame.groupby(\"Group\")[\"Family\"].ffill().bfill()\n",
    "    source_frame[\"Family\"] = source_frame[\"Family\"].fillna(source_frame.groupby(\"Group\")[\"Family\"].agg(lambda x: pd.Series.mode(x, dropna=True)))\n",
    "\n",
    "    #TODO impute missing cabins from families\n",
    "    source_frame[\"Cabin\"] = source_frame[\"Cabin\"].fillna(source_frame.groupby(\"Group\")[\"Cabin\"].agg(lambda x: pd.Series.mode(x, dropna=True)))\n",
    "    source_frame[\"Cabin\"].ffill(inplace=True)\n",
    "\n",
    "    #split cabin infor into three parts\n",
    "    source_frame[[\"Deck\", \"Num\", \"shipSide\"]] = source_frame[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "    source_frame[\"Num\"] = source_frame[\"Num\"].astype(np.float64)\n",
    "\n",
    "    #Put cabin number into bins\n",
    "    #source_frame[\"NumGroup\"] = pd.cut(source_frame[\"Num\"], bins=12)\n",
    "    \n",
    "    #create age bins\n",
    "    source_frame[\"AgeGroup\"] = np.where(source_frame[\"Age\"] <= 12, 0, 1)\n",
    "\n",
    "    #set spending for cryosleepers\n",
    "    source_frame.loc[source_frame[\"CryoSleep\"] == True ,[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]] = 0.0\n",
    "\n",
    "    # set spending of all kids to zero\n",
    "    source_frame.loc[source_frame[\"Age\"] <= 12, [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]] = 0.0\n",
    "\n",
    "    #create totalSpending column\n",
    "    source_frame[\"totalSpent\"] = source_frame[[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]].sum(axis=1)\n",
    "\n",
    "    #set age of all people not spending to average age for people 12 and under\n",
    "    source_frame[\"Age\"] = np.where((source_frame.CryoSleep == False) & (\n",
    "    source_frame.Age.isna()) & (source_frame.totalSpent == 0), 5, source_frame.Age)\n",
    "    \n",
    "    #impute VIP status by spending   \n",
    "    source_frame.loc[(source_frame.VIP.isnull()) & (source_frame.totalSpent > 3500), \"VIP\"] = True\n",
    "    source_frame[\"VIP\"].fillna(False, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    return source_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_frame_ready(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = make_frame_ready(test_data)#[[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Deck\", \"shipSide\", \"NumBin\", \"AgeBin\", \"totalSpent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = train_data[[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Deck\", \"shipSide\", \"totalSpent\", \"Transported\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.select_dtypes(object).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"HomePlanet\", \"CryoSleep\",\n",
    "            \"Destination\", \"VIP\", \"Deck\", \"shipSide\", \"AgeGroup\"]\n",
    "num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\",\"totalSpent\", \"Num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline(\n",
    "    [\n",
    "        (\"num_impute\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"num_scale\", RobustScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline(\n",
    "    [\n",
    "        (\"cat_impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"cat_encode\", OneHotEncoder())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_X = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=[\"Transported\"])\n",
    "y = train_data[\"Transported\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ct_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"auto\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 2048\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(25, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "model.add(keras.layers.Dropout(rate= 0.2))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Dense(25, activation=\"relu\"))\n",
    "\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 9)):\n",
    "        model.add(keras.layers.Dense(units=hp.Int(f\"units_{i}\", min_value=512, max_value=4196, step=512), \n",
    "                    activation=\"relu\"))\n",
    "\n",
    "        if i % 2 == 1 and hp.Boolean(\"dropout\"):\n",
    "            model.add(keras.layers.Dropout(0.15))\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    optim_choice = hp.Choice(\"optim\", [\"adam\", \"Nadam\", \"sgd\"])\n",
    "\n",
    "    if optim_choice == \"adam\":\n",
    "        optim = keras.optimizers.Adam(learning_rate=learning_rate) \n",
    "    elif optim_choice == \"Nadam\":    \n",
    "        optim = keras.optimizers.Nadam(learning_rate=learning_rate) \n",
    "    elif optim_choice == \"sgd\":    \n",
    "        optim = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        \n",
    "\n",
    "    model.compile(\n",
    "        optimizer= optim,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=model_builder,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=100,\n",
    "    overwrite=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X, y, epochs=35, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist = model.fit(X_train, y_train, epochs=1000, batch_size=4, validation_split=0.2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_in_train = sum([True for idx,row in train_data.iterrows() if any(row.isnull())])\n",
    "missing_in_test = sum([True for idx,row in test_data.iterrows() if any(row.isnull())])\n",
    "\n",
    "f\"There are {missing_in_train} missing rows in train and {missing_in_test} missing rows in test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(5)\n",
    "\n",
    "model = model_builder(best_hps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_prod = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=400, batch_size=4, validation_split=0.2, callbacks=[es_prod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ct_X.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =  model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = sub_ids.join(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = subs.rename({0: \"Transported\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs[\"Transported\"] = subs[\"Transported\"].apply(lambda x: bool(round(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs.to_csv(\"./submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14b2fa792dfd361b5372eec39c22449e7257ec4e98cdbbe646cfd3ef26cdd47c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tens-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
