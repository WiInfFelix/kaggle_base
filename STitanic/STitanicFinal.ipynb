{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "test_data = pd. read_csv(\"./test.csv\")\n",
    "\n",
    "sub_ids = test_data[\"PassengerId\"].to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 2087 missing rows in train and 996 missing rows in test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_in_train = sum([True for idx,row in train_data.iterrows() if any(row.isnull())])\n",
    "missing_in_test = sum([True for idx,row in test_data.iterrows() if any(row.isnull())])\n",
    "\n",
    "f\"There are {missing_in_train} missing rows in train and {missing_in_test} missing rows in test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frame_ready(source_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "\n",
    "    #make new column group from the passengerId\n",
    "    source_frame[\"Group\"] = source_frame[\"PassengerId\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    #make new column family from the name of the passenger\n",
    "    source_frame[\"Family\"] = source_frame[\"Name\"].apply(lambda x: str(x).split(\" \")[-1])\n",
    "\n",
    "\n",
    "    #impute missing family from group\n",
    "    #source_frame[\"Family\"] = source_frame.groupby(\"Group\")[\"Family\"].ffill().bfill()\n",
    "    source_frame[\"Family\"] = source_frame[\"Family\"].fillna(source_frame.groupby(\"Group\")[\"Family\"].agg(lambda x: pd.Series.mode(x, dropna=True)))\n",
    "\n",
    "    #TODO impute missing cabins from families\n",
    "    source_frame[\"Cabin\"] = source_frame[\"Cabin\"].fillna(source_frame.groupby(\"Group\")[\"Cabin\"].agg(lambda x: pd.Series.mode(x, dropna=True)))\n",
    "    source_frame[\"Cabin\"].ffill(inplace=True)\n",
    "\n",
    "    #split cabin infor into three parts\n",
    "    source_frame[[\"Deck\", \"Num\", \"shipSide\"]] = source_frame[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "    source_frame[\"Num\"] = source_frame[\"Num\"].astype(np.float64)\n",
    "\n",
    "    #Put cabin number into bins\n",
    "    #source_frame[\"NumGroup\"] = pd.cut(source_frame[\"Num\"], bins=12)\n",
    "    \n",
    "    #create age bins\n",
    "    source_frame[\"AgeGroup\"] = np.where(source_frame[\"Age\"] <= 12, 0, 1)\n",
    "\n",
    "    #set spending for cryosleepers\n",
    "    source_frame.loc[source_frame[\"CryoSleep\"] == True ,[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]] = 0.0\n",
    "\n",
    "    # set spending of all kids to zero\n",
    "    source_frame.loc[source_frame[\"Age\"] <= 12, [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]] = 0.0\n",
    "\n",
    "    #create totalSpending column\n",
    "    source_frame[\"totalSpent\"] = source_frame[[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]].sum(axis=1)\n",
    "\n",
    "    #set age of all people not spending to average age for people 12 and under\n",
    "    source_frame[\"Age\"] = np.where((source_frame.CryoSleep == False) & (\n",
    "    source_frame.Age.isna()) & (source_frame.totalSpent == 0), 5, source_frame.Age)\n",
    "    \n",
    "    #impute VIP status by spending   \n",
    "    source_frame.loc[(source_frame.VIP.isnull()) & (source_frame.totalSpent > 3500), \"VIP\"] = True\n",
    "    source_frame[\"VIP\"].fillna(False, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    return source_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_frame_ready(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = make_frame_ready(test_data)#[[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Deck\", \"shipSide\", \"NumBin\", \"AgeBin\", \"totalSpent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = train_data[[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Deck\", \"shipSide\", \"totalSpent\", \"Transported\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'HomePlanet',\n",
       " 'CryoSleep',\n",
       " 'Cabin',\n",
       " 'Destination',\n",
       " 'Name',\n",
       " 'Group',\n",
       " 'Family',\n",
       " 'Deck',\n",
       " 'shipSide']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.select_dtypes(object).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"HomePlanet\", \"CryoSleep\",\n",
    "            \"Destination\", \"VIP\", \"Deck\", \"shipSide\", \"AgeGroup\", \"NumGroup\"]\n",
    "num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\",\"totalSpent\", \"Num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline(\n",
    "    [\n",
    "        (\"num_impute\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"num_scale\", RobustScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline(\n",
    "    [\n",
    "        (\"cat_impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"cat_encode\", OneHotEncoder())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_X = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=[\"Transported\"])\n",
    "y = train_data[\"Transported\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NumGroup'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/sklearn/utils/__init__.py:416\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m columns:\n\u001b[0;32m--> 416\u001b[0m     col_idx \u001b[39m=\u001b[39m all_columns\u001b[39m.\u001b[39;49mget_loc(col)\n\u001b[1;32m    417\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col_idx, numbers\u001b[39m.\u001b[39mIntegral):\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NumGroup'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/kaggle_base/STitanic/STitanicFinal.ipynb Zelle 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bwiinffelix-kaggle-base-4gwq54jvfqp75/workspaces/kaggle_base/STitanic/STitanicFinal.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m ct_X\u001b[39m.\u001b[39;49mfit_transform(X)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:670\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    669\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 670\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_column_callables(X)\n\u001b[1;32m    671\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    673\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:357\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m         columns \u001b[39m=\u001b[39m columns(X)\n\u001b[1;32m    356\u001b[0m     all_columns\u001b[39m.\u001b[39mappend(columns)\n\u001b[0;32m--> 357\u001b[0m     transformer_to_input_indices[name] \u001b[39m=\u001b[39m _get_column_indices(X, columns)\n\u001b[1;32m    359\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_columns \u001b[39m=\u001b[39m all_columns\n\u001b[1;32m    360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transformer_to_input_indices \u001b[39m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/sklearn/utils/__init__.py:424\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    421\u001b[0m             column_indices\u001b[39m.\u001b[39mappend(col_idx)\n\u001b[1;32m    423\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 424\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mA given column is not a column of the dataframe\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[39mreturn\u001b[39;00m column_indices\n\u001b[1;32m    427\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "X = ct_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"auto\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 2048\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(25, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "model.add(keras.layers.Dropout(rate= 0.2))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Dense(25, activation=\"relu\"))\n",
    "\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 9)):\n",
    "        model.add(keras.layers.Dense(units=hp.Int(f\"units_{i}\", min_value=512, max_value=4196, step=512), \n",
    "                    activation=\"relu\"))\n",
    "\n",
    "        if i % 2 == 1 and hp.Boolean(\"dropout\"):\n",
    "            model.add(keras.layers.Dropout(0.15))\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    optim_choice = hp.Choice(\"optim\", [\"adam\", \"Nadam\", \"sgd\"])\n",
    "\n",
    "    if optim_choice == \"adam\":\n",
    "        optim = keras.optimizers.Adam(learning_rate=learning_rate) \n",
    "    elif optim_choice == \"Nadam\":    \n",
    "        optim = keras.optimizers.Nadam(learning_rate=learning_rate) \n",
    "    elif optim_choice == \"sgd\":    \n",
    "        optim = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        \n",
    "\n",
    "    model.compile(\n",
    "        optimizer= optim,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=model_builder,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=100,\n",
    "    overwrite=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X, y, epochs=35, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist = model.fit(X_train, y_train, epochs=1000, batch_size=4, validation_split=0.2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_in_train = sum([True for idx,row in train_data.iterrows() if any(row.isnull())])\n",
    "missing_in_test = sum([True for idx,row in test_data.iterrows() if any(row.isnull())])\n",
    "\n",
    "f\"There are {missing_in_train} missing rows in train and {missing_in_test} missing rows in test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(5)\n",
    "\n",
    "model = model_builder(best_hps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_prod = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=400, batch_size=4, validation_split=0.2, callbacks=[es_prod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ct_X.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =  model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = sub_ids.join(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = subs.rename({0: \"Transported\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs[\"Transported\"] = subs[\"Transported\"].apply(lambda x: bool(round(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs.to_csv(\"./submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
